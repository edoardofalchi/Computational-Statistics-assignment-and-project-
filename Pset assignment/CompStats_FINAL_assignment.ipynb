{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gby7PCrSLVEV"
   },
   "source": [
    "# Computational Statistics, SS 2021\n",
    "-------------------------------------------------------------------------------------\n",
    "## Problem Set  - solutions\n",
    "\n",
    "### Edoardo Falchi, Wilko Oltmanns and Carolina √Ålvarez\n",
    "******************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kyiwxz45LVEd"
   },
   "outputs": [],
   "source": [
    "set.seed(321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfpMZ65HLVEe"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "**Consider the linear regression model:**\n",
    "\n",
    "$$\n",
    "y= X\\beta + \\varepsilon, \\quad \\text{with} \\quad \\varepsilon \\sim \\mathcal{N}_{n}(0, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "$ \\textstyle X_{1}$ is a constant, $\\textstyle X_{2} \\sim \\mathcal{N}(\\mu=0, \\sigma^2=1.5)$. The error term is generated as $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^2=10)$. The true Data Generating Process uses as $\\beta=(5, -0.5)$ and $N = 1000$.\n",
    "\n",
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HaRNA4livov"
   },
   "source": [
    "First of all, let's define some functions that will turn out be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9wYqAdb8jNOx"
   },
   "outputs": [],
   "source": [
    "#--------------The data generating process (DGP)-------------------------------------------------------\n",
    "data_generator <- function(N, beta){\n",
    "  ##\n",
    "  X   <- cbind(rep(1, N), rnorm(N, 0, sqrt(1.5)))\n",
    "  ##\n",
    "  eps  <- rnorm(N, 0, sqrt(10))\n",
    "  Y    <- X %*% beta + eps\n",
    "  data <- data.frame(\"Y\"=Y, \"X\"=X[,2])\n",
    "  ##\n",
    "  return(data)\n",
    "}\n",
    "\n",
    "\n",
    "#---------------Mean Squared Error Calculator----------------------------------------------------------\n",
    "mse <- function(lm){\n",
    "  mean(lm$residuals^2)\n",
    "}\n",
    "\n",
    "#----------------Average prediction error Calculator----------------------------------------------------------\n",
    "a_pred_error <- function(data_test, beta_hat, N){\n",
    "  \n",
    "  X <- as.matrix(cbind(rep(1, N), data_test[-1]))\n",
    "  y_pred <-  X %*% beta_hat\n",
    "  mean((data_test[,1] - y_pred)^2)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytcvTBQCiohO"
   },
   "source": [
    "**a) Generating a training sample $\\lbrace(x_{i},y_{i})\\rbrace_{i=1}^N $ using the above specification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VhmFAyxDLVEf",
    "outputId": "a9efd74b-4f86-4312-8ada-2e12f169d161"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Y</th><th scope=col>X</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>-0.7760613</td><td> 2.0880715</td></tr>\n",
       "\t<tr><td> 1.3708686</td><td>-0.8720656</td></tr>\n",
       "\t<tr><td> 9.6058622</td><td>-0.3404606</td></tr>\n",
       "\t<tr><td> 5.1639138</td><td>-0.1465395</td></tr>\n",
       "\t<tr><td>11.0866836</td><td>-0.1518201</td></tr>\n",
       "\t<tr><td> 1.3222990</td><td> 0.3284567</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Y & X\\\\\n",
       "\\hline\n",
       "\t -0.7760613 &  2.0880715\\\\\n",
       "\t  1.3708686 & -0.8720656\\\\\n",
       "\t  9.6058622 & -0.3404606\\\\\n",
       "\t  5.1639138 & -0.1465395\\\\\n",
       "\t 11.0866836 & -0.1518201\\\\\n",
       "\t  1.3222990 &  0.3284567\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Y | X |\n",
       "|---|---|\n",
       "| -0.7760613 |  2.0880715 |\n",
       "|  1.3708686 | -0.8720656 |\n",
       "|  9.6058622 | -0.3404606 |\n",
       "|  5.1639138 | -0.1465395 |\n",
       "| 11.0866836 | -0.1518201 |\n",
       "|  1.3222990 |  0.3284567 |\n",
       "\n"
      ],
      "text/plain": [
       "  Y          X         \n",
       "1 -0.7760613  2.0880715\n",
       "2  1.3708686 -0.8720656\n",
       "3  9.6058622 -0.3404606\n",
       "4  5.1639138 -0.1465395\n",
       "5 11.0866836 -0.1518201\n",
       "6  1.3222990  0.3284567"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting parameters for the training sample\n",
    "N <- 1000\n",
    "beta_true <- c(5, -0.5)\n",
    "training_sample <- data_generator(N = N, beta = beta_true)\n",
    "head(training_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0dwoEbzLVEg"
   },
   "source": [
    "**b) Generate a test sample $\\lbrace(x^{\\prime}_{i},y^{\\prime}_{i})\\rbrace_{i=1}^N $**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xr93ShL5LVEg",
    "outputId": "ff2765de-0f38-45f8-c935-2a7f1b11440c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Y</th><th scope=col>X</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>2.755092  </td><td>-1.2442022</td></tr>\n",
       "\t<tr><td>6.882837  </td><td> 0.8692187</td></tr>\n",
       "\t<tr><td>5.407237  </td><td> 0.7419291</td></tr>\n",
       "\t<tr><td>8.934161  </td><td> 1.6934456</td></tr>\n",
       "\t<tr><td>1.222592  </td><td>-1.8001885</td></tr>\n",
       "\t<tr><td>4.135590  </td><td> 1.0395220</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Y & X\\\\\n",
       "\\hline\n",
       "\t 2.755092   & -1.2442022\\\\\n",
       "\t 6.882837   &  0.8692187\\\\\n",
       "\t 5.407237   &  0.7419291\\\\\n",
       "\t 8.934161   &  1.6934456\\\\\n",
       "\t 1.222592   & -1.8001885\\\\\n",
       "\t 4.135590   &  1.0395220\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Y | X |\n",
       "|---|---|\n",
       "| 2.755092   | -1.2442022 |\n",
       "| 6.882837   |  0.8692187 |\n",
       "| 5.407237   |  0.7419291 |\n",
       "| 8.934161   |  1.6934456 |\n",
       "| 1.222592   | -1.8001885 |\n",
       "| 4.135590   |  1.0395220 |\n",
       "\n"
      ],
      "text/plain": [
       "  Y        X         \n",
       "1 2.755092 -1.2442022\n",
       "2 6.882837  0.8692187\n",
       "3 5.407237  0.7419291\n",
       "4 8.934161  1.6934456\n",
       "5 1.222592 -1.8001885\n",
       "6 4.135590  1.0395220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the test sample\n",
    "test_sample <- data_generator(N = N, beta = beta_true)\n",
    "head(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUR7ODeOLVEh"
   },
   "source": [
    "**c) Calculate the OLS estimate for $\\hat{\\beta}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QoTw9BcLVEh",
    "outputId": "a7964506-4ee3-4241-9c9d-074feddbb958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Intercept)           X \n",
      "  5.0497801  -0.5093877 \n"
     ]
    }
   ],
   "source": [
    "lm_training <- lm(Y ~ X, data = training_sample)\n",
    "beta_hat <- coef(lm_training) #storing the estimates for beta\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fziL3n2uLVEh"
   },
   "source": [
    "**d) Calculate the training MSE and the prediction error using the expressions given below for these two individual samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrwhHZZ6LVEi",
    "outputId": "2f256f17-39ea-4b77-9511-3a014dc800cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 10.25031\n",
      "[1] 9.781445\n"
     ]
    }
   ],
   "source": [
    "MSE <- mse(lm = lm_training)\n",
    "print(MSE)\n",
    "\n",
    "pred_error <- a_pred_error(data_test = test_sample, beta_hat = beta_hat, N = N)\n",
    "print(pred_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx8fYykfLVEi"
   },
   "source": [
    "**e) Using the training and the test samples from above, calculate the training MSE and the avg. prediction error when sequentially increasing the degree of the polynomial for $X_{2}$ from zero (constant only) to five in the estimation equation (i.e. include $X^2_{2}, X^3_{2}, X^4_{2}, X^5_{2}$ as regressors).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to estimate the following regressions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " y &= \\beta_1X_{1} + \\varepsilon \\\\\n",
    " y &= \\beta_1X_{1} + \\beta_2X_{2} +\\varepsilon \\\\\n",
    " y &= \\beta_1X_{1} + \\beta_2X_{2} + \\beta_3X^2_{2} +\\varepsilon \\\\\n",
    " &\\vdots\\\\\n",
    " y &= \\beta_1X_{1} + \\beta_2X_{2} + \\beta_3X^2_{2} + \\beta_4X^3_{2} + \\beta_5X^4_{2} + \\beta_6X^5_{2} + \\varepsilon\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBuIm5MOLVEj",
    "outputId": "0faee172-6d3a-4c99-927a-fff4af9b13eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 10.63217 10.25031 10.24857 10.24528 10.24361 10.21423\n",
      "[1] 10.075501  9.781445  9.781147  9.806688  9.795588  9.839813\n"
     ]
    }
   ],
   "source": [
    "X_power_training <- matrix(NaN,N,5) #We want polynomials with degrees up to 5\n",
    "X_power_test <- matrix(NaN,N,5)\n",
    "\n",
    "# We take X (second column of the sample) and take it to the power of r. Then we store it. \n",
    "\n",
    "\n",
    "for (r in 1:5){\n",
    "  \n",
    "  X_power_training[,r] <- training_sample[,2]^r \n",
    "  X_power_test[,r] <- test_sample[,2]^r\n",
    "  \n",
    "}\n",
    "\n",
    "# We store the new generated variables together with the realizations of Y in a data frame to \n",
    "# fit a linear model and calculate errors (again for both samples). \n",
    "\n",
    "\n",
    "data_power_training <- data.frame(\"Y\" = training_sample[,1], X_power_training)\n",
    "data_power_test <- data.frame(\"Y\" = test_sample[,1], X_power_test)\n",
    "\n",
    "#----------------------Fitting polynomial models with different degrees------------------------\n",
    "\n",
    "\n",
    "# We are running 6 linear regression with polynomials up to degree 5, calculating the MSE\n",
    "# and the average prediction error for each result. \n",
    "\n",
    "#Generating vectors to store the results\n",
    "\n",
    "MSE_result <- c(NaN)\n",
    "pred_error_result <- c(NaN)\n",
    "\n",
    "# We are considering the special case with just the constant (degree 0) outside of the loop\n",
    "\n",
    "l_model <- lm(Y ~ 1, data = data_power_training)\n",
    "\n",
    "MSE_result[1] <- mse(lm = l_model) \n",
    "pred_error_result[1] <- mean((data_power_test[,1] - coef(l_model))^2)\n",
    "\n",
    "\n",
    "for (r in 1:5){ # r is the maximal degree of the polynomials\n",
    "  \n",
    "  l_model <- lm(data_power_training[,1] ~ X_power_training[,1:r])\n",
    "  MSE_result[r+1] <- mse(lm = l_model)\n",
    "  pred_error_result[r+1] <- a_pred_error(data_test = data_power_test[,1:(r+1)],\n",
    "                                         beta_hat = coef(l_model),\n",
    "                                         N = N)\n",
    "}  \n",
    "\n",
    "print(MSE_result)\n",
    "print(pred_error_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNeFzUbeLVEk"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "**Using the general set-up from above**\n",
    "\n",
    "**a) Repeat the simulation 1000 times, each time setting the seed at $100+$ the number of the simulation run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-vUQ6xLRLVEk"
   },
   "outputs": [],
   "source": [
    "num_sim <- 1000  #number of repetitions\n",
    "result_MSE <- data.frame(c(rep(NaN,6))) #data frames for our results\n",
    "result_pred <- data.frame(c(rep(NaN,6)))\n",
    "\n",
    "\n",
    "for (r in 1:num_sim){\n",
    "  set.seed(100 + r)\n",
    "\n",
    "  training_sample <- data_generator(N = N, beta = beta_true) #Drawing test and training sample\n",
    "  test_sample <- data_generator(N = N, beta = beta_true)\n",
    "  \n",
    "  #---------------------------------------------------------------\n",
    "  \n",
    "  X_power_training <- matrix(NaN,N,5) #Generating new variables\n",
    "  X_power_test <- matrix(NaN,N,5)\n",
    "  \n",
    "  for (i in 1:5){\n",
    "    X_power_training[,i] <- training_sample[,2]^i \n",
    "    X_power_test[,i] <- test_sample[,2]^i\n",
    "  }\n",
    "  \n",
    "  data_power_training <- data.frame(\"Y\" = training_sample[,1], X_power_training) \n",
    "  data_power_test <- data.frame(\"Y\"=test_sample[,1], X_power_test)\n",
    "  \n",
    "  #---------------------------------------------------------------\n",
    "  \n",
    "  #Fitting models for polynomials with different degrees\n",
    "  \n",
    "  MSE_result <- c(NaN)\n",
    "  pred_error_result <- c(NaN)\n",
    "  \n",
    "  l_model <- lm(Y ~ 1, data = data_power_training) #special case\n",
    "  \n",
    "  MSE_result[1] <- mse(lm = l_model) \n",
    "  pred_error_result[1] <- mean((data_power_test[,1] - coef(l_model))^2)\n",
    "  \n",
    "  \n",
    "  for (k in 1:5){ \n",
    "    \n",
    "    l_model <- lm(data_power_training[,1] ~ X_power_training[,1:k])\n",
    "    MSE_result[k+1] <- mse(lm = l_model)\n",
    "    pred_error_result[k+1] <- a_pred_error(data_test = data_power_test[,1:(k+1)],\n",
    "                                           beta_hat = coef(l_model),\n",
    "                                           N = N)\n",
    "  }  \n",
    "  \n",
    "  result_MSE[r] <- MSE_result #storing the results\n",
    "  result_pred[r] <- pred_error_result\n",
    "  \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6rorkYwLVEl"
   },
   "source": [
    "**b) Calculate the average training MSE and the average prediction error using the expressions given below and store the results in a vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JW9UZ3lrLVEl",
    "outputId": "802b4886-f015-4b3f-ee23-a988121931cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 10.355226  9.969655  9.959822  9.949304  9.938792  9.928603\n",
      "[1] 10.38791 10.02295 10.03371 10.04561 10.06396 10.10037\n"
     ]
    }
   ],
   "source": [
    "MSE_mean_sim <- rowMeans(result_MSE)\n",
    "pred_mean_sim <- rowMeans(result_pred)\n",
    "\n",
    "print(MSE_mean_sim)\n",
    "print(pred_mean_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlsMd0e2LVEm"
   },
   "source": [
    "**c) Plot the avg. training MSE and the avg. prediction error in one plot and discuss your results. Be sure to complete this simulation for the set-up described in 1 e).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "n0-8sw18LVEm",
    "outputId": "d4c49ff3-49e1-4071-c3cd-767a2c7fd87b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.3\""
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAHgCAMAAABdO/S2AAAAPFBMVEUAAAAAv8QzMzNNTU1o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\nGFCMgAHFCBhQjIABxQgYUOx/OpnqlAwyyQYAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#! install.packages(\"tidyverse\")\n",
    "library(ggplot2)\n",
    "library(repr)\n",
    "# Change plot size to 8 x 4\n",
    "options(repr.plot.width=8, repr.plot.height=4)\n",
    "\n",
    "data = data.frame(c(0:5),MSE_mean_sim,pred_mean_sim)\n",
    "\n",
    "ggplot(data = data, aes()) + \n",
    "  geom_point(mapping = aes(x = data[,1] , y = MSE_mean_sim, color = \"red\" )) +\n",
    "  geom_point(mapping = aes(x = data[,1],y = pred_mean_sim, color = \"blue\")) +\n",
    "  geom_line(mapping = aes(x = data[,1] , y = MSE_mean_sim, color = \"red\" )) +\n",
    "  geom_line(mapping = aes(x = data[,1],y = pred_mean_sim, color = \"blue\")) +\n",
    "  ggtitle(\"Average MSE and prediction error\") + \n",
    "  theme(plot.title = element_text(hjust = 0.5)) +\n",
    "  xlab(\"Degree of polynomial\") + ylab(\"MSE, APE\") +\n",
    "  scale_color_discrete(name=\"\", labels = c(\"APE\", \"MSE\")) +\n",
    "  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "        panel.background = element_blank(), axis.line = element_line(colour = \"black\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in our simulation study, that on average the MSE decreases and the APE increases, when we increase the degree of the polynomial (after the constant case). When you increase the degrees, the fit of the model to the training-data-set gets better and better. This is so, because the OLS method minimizes the sum of the squared residuals, which is N*MSE. When we increase the number of parameters, the MSE is at least as high (then the coefficient of this new parameter is equal to zero), or the sum is smaller than before. \n",
    "\n",
    "So the MSE is always decreasing, but we don't care so much about the MSE, because to get the smallest MSE we would just set the predicted y-values equal to the observed ones, leading to an MSE equal to zero. Instead we are interested, how good the model is in predicting values, not used to fit the model. This is captured in the average prediction error. When we increase the degree of the polynomials, then the average prediction error gets bigger and bigger, leading (on average) to less and less accurate predictions, because we overfitted the model. \n",
    "\n",
    "Therefore, we can conclude, that the MSE is a measure for how close the fit of the model is to the training data, but can't be used to assess the accuracy of the predictions from this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6CWpwg2LVEn"
   },
   "source": [
    "**d) Along which margins could you vary parameters of the initial simulation set-up and what would be your intuition based on the theoretical properties of the considered objects of interest?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUXAl219pX0I"
   },
   "source": [
    "Our data generating process fulfills the all four assumptions from the lecture. Therefore, the OLS-estimator, which is used in the lm_function, is an unbiased estimator. This means, that (on average), our estimation-results are close to the theoretical (true) parameters in the background. In this setting, the OLS estimator is also a consistent estimator. When we would increase the sample size, the accuracy of our prediction would (on average) get better and better, due to the Central Limit Theorem. With N=1000 we have already a quite large sample. This explains, why the estimated coefficients above were close to the coefficients from the data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNV4UeFvp6RQ",
    "outputId": "9d3e6625-8d4f-4d6e-c978-94c96147a62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model coefficients \n",
      "(Intercept)           X \n",
      "  5.0497801  -0.5093877 \n",
      "\n",
      "True beta \n",
      "[1]  5.0 -0.5\n"
     ]
    }
   ],
   "source": [
    "cat(\"training model coefficients \\n\")\n",
    "print(coef(lm_training)) \n",
    "cat(\"\\n\")\n",
    "cat(\"True beta \\n\")\n",
    "print(beta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_hOBFYzse_O"
   },
   "source": [
    "An other observation is, that the MSE is roughly equal to 10, which is also the value of the variance of the error term in our data generating process. This is not a coincidence, because with this setup,the MSE is an asymptotically unbiased estimator of the error-variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIoyQM2Rsm10"
   },
   "source": [
    "# Reference\n",
    "* James, G., D. Witten, T. Hastie, and R. Tibshirani (2013). [An introduction to statistical learning](https://link.springer.com/book/10.1007/978-1-4614-7138-7), vol. 112, Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDoc8qOcLVEp"
   },
   "source": [
    "<ins>Training MSE</ins>\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i = 1}^n \\big(y_i-\\hat{\\rho}(\\mathbf{x}_i)\\big)^2\n",
    "$$\n",
    "\n",
    "where $\\hat{\\rho}(\\mathbf{x}_i)$ is the prediction $\\hat{\\rho}$ gives for the $i$th observation.\n",
    "\n",
    "<ins>Average prediction error</ins>\n",
    "\n",
    "$$\n",
    "\\text{Ave} \\big(y_i^\\prime-\\hat{\\rho}(\\mathbf{x}^\\prime_i)\\big)^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PS1_CompStats.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
